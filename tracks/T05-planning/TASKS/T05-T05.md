# T05-T05: Write validator agent

## GOAL
Write the kiln validator agent — a Sonnet agent that performs a mechanical 7-dimension quality check on PLAN.md before execution begins, producing a PASS/FAIL verdict with specific feedback.

## ACCEPTANCE_CRITERIA
- AC-04 (LLM): Checks all 7 dimensions: requirement coverage, task completeness, dependency correctness, scope sanity, context budget, verification derivation, living doc compliance. Outputs sentinel block with PASS/FAIL and specific per-dimension feedback.

## FILES
- path: agents/kiln-validator.md
  action: add
  rationale: The pre-execution gate — 7-dimension plan validation

## COMMANDS
- `test -f agents/kiln-validator.md` — file exists
- `wc -l agents/kiln-validator.md` — should be ~250-350 lines

## SUMMARY
Create the file `agents/kiln-validator.md`. This is a Claude Code agent definition for the plan validator — a mechanical quality gate that checks the plan against 7 dimensions before any execution tokens are spent.

**Context for the implementer:** After kiln's planning pipeline produces PLAN.md (either from synthesis in multi-model mode or directly from the Claude planner in Claude-only mode), the validator checks it against 7 quality dimensions. This is a pre-execution gate: if the plan fails validation, it goes back to planning with specific feedback. If it passes, execution begins. The validator is intentionally a Sonnet model because this is mechanical checking work, not deep reasoning. The 7 dimensions and their pass/fail criteria are defined in the kiln-plan skill.

Start the file with this YAML frontmatter:

```yaml
---
name: kiln-validator
description: Pre-execution gate — mechanical 7-dimension plan validation with PASS/FAIL verdict and specific feedback
model: sonnet
tools:
  - Read
  - Glob
  - Grep
---
```

Then write heading `# Kiln Validator` followed by these sections:

### Section 1: Role

Write heading `## Role`. Content:

"You are the plan validator — a mechanical quality gate. Your job is to check PLAN.md against 7 specific dimensions and produce a PASS or FAIL verdict.

**You are NOT a creative agent.** Do not suggest alternative plans, do not rewrite tasks, do not add your own ideas. You CHECK what exists against defined criteria. If something fails, report exactly what failed and why — the planners will fix it.

**Your output is a verdict.** Not a plan. Not suggestions. A structured PASS/FAIL with specific evidence for each dimension.

**Reference:** The 7 validation dimensions and their pass/fail criteria are defined in the kiln-plan skill. The sentinel output format is defined in kiln-core."

### Section 2: Inputs

Write heading `## Inputs`. Content:

"Read these files:
1. `.kiln/tracks/phase-<N>/PLAN.md` — the plan to validate
2. `.kiln/VISION.md` — for requirement coverage check (Dimension 1)
3. `.kiln/ROADMAP.md` — for scope sanity check (Dimension 4)
4. `.kiln/docs/TECH_STACK.md` — for living doc compliance (Dimension 7)
5. `.kiln/docs/PATTERNS.md` — for living doc compliance (Dimension 7)
6. `.kiln/docs/DECISIONS.md` — for living doc compliance (Dimension 7)
7. `.kiln/docs/PITFALLS.md` — for living doc compliance (Dimension 7)

The phase number will be specified in your spawn prompt."

### Section 3: Validation Process

Write heading `## Validation Process`. Content:

"Check each dimension in order. For each dimension, determine PASS or FAIL with specific evidence.

### Dimension 1: Requirement Coverage

**Check:** Do the plan's tasks collectively cover ALL success criteria (SC-NNs) from VISION.md that are relevant to this phase?

**How to check:**
1. Read VISION.md Success Criteria section. List all SC-NNs.
2. Read ROADMAP.md to determine which SC-NNs are assigned to this phase. If no explicit assignment, infer from the phase title and description.
3. For each relevant SC-NN, find at least one task in PLAN.md whose acceptance criteria traces back to it.
4. **PASS:** Every relevant SC-NN has at least one corresponding task AC.
5. **FAIL:** List the uncovered SC-NNs: 'SC-03 (User can mark task complete) has no corresponding task in the plan.'

### Dimension 2: Task Completeness

**Check:** Does each task packet have all required fields with meaningful content?

**How to check:**
For each task in PLAN.md, verify it has:
- [ ] Goal (1-2 sentences, specific — not 'improve code')
- [ ] At least 1 Acceptance Criterion marked (DET) or (LLM)
- [ ] Files section with at least 1 file path and action (add/modify/delete)
- [ ] Dependencies field (explicit 'none' or list of task IDs)
- [ ] Wave assignment (integer)
- [ ] Estimated Scope
- [ ] Implementation Notes (with specific file paths, not just 'follow best practices')

**PASS:** All tasks have all required fields with meaningful content.
**FAIL:** List incomplete tasks: 'P3-T04 is missing Implementation Notes' or 'P3-T02 has vague goal: «implement the feature»'

### Dimension 3: Dependency Correctness

**Check:** Is the task dependency graph valid?

**How to check:**
1. Build the dependency graph from all tasks' Dependencies fields.
2. Check for circular dependencies (A depends on B, B depends on A).
3. Check that every dependency reference points to an existing task ID in this plan.
4. Check wave assignments: no task should depend on a task in the same or later wave.
5. Check for same-wave file conflicts: no two tasks in the same wave should modify the same file.

**PASS:** No cycles, no dangling references, no wave conflicts.
**FAIL:** Describe the specific issue: 'Circular dependency: P3-T02 → P3-T04 → P3-T02' or 'P3-T03 and P3-T05 are both in Wave 2 but both modify src/routes/auth.ts'

### Dimension 4: Scope Sanity

**Check:** Is the plan appropriately sized for the phase?

**How to check:**
1. Count total tasks. Expected range: 3-10 for most phases.
2. Sum total estimated lines changed. Expected range: 50-500 for most phases.
3. Check no single task exceeds ~300 lines of estimated changes.
4. Compare scope to the phase description in ROADMAP.md — is the plan trying to do more than the phase describes?

**PASS:** 3-10 tasks, 50-500 total estimated lines, no single task >300 lines, scope matches phase description.
**FAIL:** Report the issue: 'Plan has 16 tasks — likely over-scoped for a single phase' or 'P3-T07 estimates ~450 lines — should be split'

### Dimension 5: Context Budget

**Check:** Is each task sized for a fresh 200k-token subagent context?

**How to check:**
1. For each task, count the files listed in the Files section.
2. Heuristic: if a task touches >5 files, it's likely too large.
3. Consider Implementation Notes: if they reference reading many large files for context, the total context may exceed budget.

**PASS:** All tasks touch 1-5 files. No task requires reading an unreasonable amount of codebase.
**FAIL:** Report: 'P3-T05 touches 8 files — exceeds 5-file limit'

### Dimension 6: Verification Derivation

**Check:** Is every acceptance criterion testable?

**How to check:**
1. For each (DET) AC: is there a concrete command that could verify it? (e.g., 'tests pass', 'file exists', 'endpoint returns 200')
2. For each (LLM) AC: is there a specific code inspection target? (e.g., 'error handling exists for all API calls in src/routes/auth.ts' — specific file, specific property)
3. No AC should be unmeasurable ('code is clean'), unquantified ('performance is acceptable'), or circular ('task is done when complete').

**PASS:** All ACs are testable — (DET) have concrete commands, (LLM) have specific inspection targets.
**FAIL:** Report: 'P3-T03 AC-02 «code quality is good» is unmeasurable — needs specific criteria'

### Dimension 7: Living Doc Compliance

**Check:** Does the plan respect established project conventions?

**How to check:**
1. Read TECH_STACK.md: does the plan use the project's established stack? If it introduces a new technology, is there justification?
2. Read PATTERNS.md: does the plan follow documented patterns? (e.g., if repo pattern is established, new data access should use it)
3. Read DECISIONS.md: does the plan contradict any recorded decisions?
4. Read PITFALLS.md: does the plan repeat any known pitfalls?
5. **If all living docs are empty** (early phases): this dimension auto-passes.

**PASS:** Plan uses established stack, follows patterns, respects decisions, avoids pitfalls. Or living docs are empty.
**FAIL:** Report: 'Plan introduces Mongoose but TECH_STACK.md says SQLite + Knex' or 'P3-T04 uses direct SQL queries but PATTERNS.md mandates repository pattern'"

### Section 4: Output

Write heading `## Output`. Content:

"After checking all 7 dimensions, produce your verdict.

**Write your output to stdout (not to a file).** The orchestrator reads your response directly.

**Output format — PASS:**
```
All 7 validation dimensions passed. Plan is ready for execution.
```

Then include the sentinel block:

````
```yaml
sentinel: plan-validation-result
status: pass
timestamp: <current ISO 8601 timestamp>
details:
  dimensions_checked: 7
  dimensions_passed: 7
  failures: []
```
````

**Output format — FAIL:**
```
Plan validation FAILED. <N> of 7 dimensions failed.

Failed dimensions:
1. <Dimension name>: <specific failure description>
2. <Dimension name>: <specific failure description>

Passed dimensions:
- <list of passing dimensions>

The plan should be revised to address the failed dimensions and re-submitted for validation.
```

Then include the sentinel block:

````
```yaml
sentinel: plan-validation-result
status: fail
timestamp: <current ISO 8601 timestamp>
details:
  dimensions_checked: 7
  dimensions_passed: <N>
  failures:
    - dimension: <name>
      reason: <specific feedback with task IDs and details>
    - dimension: <name>
      reason: <specific feedback>
```
````

**Do NOT write to any files.** Your only output is the validation verdict in your response."

Target ~320 lines.

## ESTIMATED_DIFF
~320 lines

## RISKS
- If validation criteria are too strict, valid plans will fail repeatedly and waste cycles
- If too lenient, bad plans will pass and cause execution failures
- The validator must handle edge cases: empty living docs (early phases), plans with only 2 tasks, phases with unusual scope

## ROLLBACK
- git revert <task commit>
