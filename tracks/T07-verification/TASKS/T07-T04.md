# T07-T04: Write verification skill

## GOAL
Write the kiln verification skill — the adaptive verification protocol that detects project tooling, runs deterministic verification tools, layers AI goal-backward verification on top, and provides a comprehensive stub detection checklist. This skill is referenced by both the E2E verifier and reviewer agents.

## ACCEPTANCE_CRITERIA
- AC-04 (LLM): Stub detection covers: null-returning components, hardcoded API responses, no-op form handlers, unhandled fetch responses, console.log-only functions
- AC-06 (DET): File exists at `skills/kiln-verify/kiln-verify.md` with valid YAML frontmatter
- AC-11 (LLM): Adaptive verification protocol defines 3 layers: tooling detection, tool execution, AI verification
- AC-12 (LLM): Verification result format defined with pass/fail and evidence

## FILES
- path: skills/kiln-verify/kiln-verify.md
  action: add
  rationale: Adaptive verification protocol and stub detection patterns for the verification layer

## COMMANDS
- `test -f skills/kiln-verify/kiln-verify.md` — file exists
- `head -1 skills/kiln-verify/kiln-verify.md | grep -q "^---"` — has YAML frontmatter
- `wc -l skills/kiln-verify/kiln-verify.md` — should be ~200-300 lines

## SUMMARY
Create the file `skills/kiln-verify/kiln-verify.md`. This is a Claude Code skill definition — a markdown document with YAML frontmatter that provides verification protocols and patterns.

**Context for the implementer:** This skill is the METHODOLOGY reference for verification in kiln. It defines HOW to verify, not WHAT to verify (acceptance criteria define WHAT). It is referenced by both the E2E verifier agent (`agents/kiln-e2e-verifier.md`) and the reviewer agent (`agents/kiln-reviewer.md`). The adaptive verification protocol means: first detect what tools exist, then run those tools, then layer AI judgment on top for things tools can't check. The stub detection checklist is a critical component — it catches incomplete implementations that pass tests but don't actually work.

Start the file with this YAML frontmatter:

```yaml
---
name: kiln-verify
description: "Adaptive verification protocol — tooling detection, deterministic verification, AI goal-backward verification, and stub detection"
---
```

Then write heading `# Kiln Verification Protocol` followed by these sections:

### Section 1: Adaptive Verification Overview
Write heading `## Adaptive Verification`. Explain the 3-layer approach:
- Layer 1: Detect what verification tools the project has (from `.kiln/config.json` tooling field)
- Layer 2: Run detected tools deterministically (test runner, linter, type checker)
- Layer 3: Layer AI goal-backward verification for things tools can't check
- Principle: deterministic checks are TRUTH. AI judgment supplements but never overrides deterministic results.
- Order matters: run deterministic first, then AI. A linter failure is more trustworthy than an LLM opinion.

### Section 2: Layer 1 — Tooling Detection
Write heading `## Layer 1: Tooling Detection`. Define:
- Read `.kiln/config.json` then `tooling` object
- Detection for each tool type:
  - `testRunner`: jest, vitest, mocha, pytest, go test, cargo test — the config field contains the actual command
  - `linter`: eslint, ruff, golangci-lint, clippy — ditto
  - `typeChecker`: tsc --noEmit, mypy, pyright — ditto
  - `buildSystem`: webpack, vite, esbuild, tsc, cargo build — ditto
- If a tool is not detected (null in config.json), skip that layer. Don't fail.
- Tool detection happens at init time (kiln-init skill). The verification skill just reads the config.

### Section 3: Layer 2 — Deterministic Verification
Write heading `## Layer 2: Deterministic Verification`. Define:
- Run each detected tool in order: build then type check then lint then test
- For each tool run:
  - Capture exit code, stdout, stderr
  - Parse results where possible (test count, pass/fail, error lines)
  - On failure: extract specific error locations (file:line where possible)
- Result format per tool:
  ```
  Tool: <name>
  Command: <exact command>
  Status: PASS|FAIL
  Summary: <brief result>
  Errors: [file:line: message, ...]
  ```
- Deterministic results are AUTHORITATIVE. If the linter says there's an error, there IS an error.

### Section 4: Layer 3 — AI Goal-Backward Verification
Write heading `## Layer 3: AI Goal-Backward Verification`. Define:
- After deterministic tools run, apply AI judgment for criteria that tools can't check:
  - Does the code achieve the GOAL stated in the acceptance criteria?
  - Is the implementation COMPLETE (not just syntactically correct)?
  - Are there stubs or placeholders that pass tests but don't do real work?
  - Does the code integrate properly with the rest of the system?
- Goal-backward means: start from the acceptance criterion, trace backward to the code that implements it. Verify the chain is complete.
- AI verification is SUPPLEMENTARY. It catches things tools miss. It does not override tool results.

### Section 5: Stub Detection Checklist
Write heading `## Stub Detection Checklist`. This is CRITICAL. Provide a detailed checklist with 10 items:

1. **Null/empty returns**: Components that always return null, undefined, empty array, empty object, or empty string. Check: is there a code path where real data is returned?
2. **Hardcoded responses**: API handlers that return static JSON instead of querying a data source. Check: is there a database/service call in the handler?
3. **No-op form handlers**: Forms with onSubmit that only call preventDefault and nothing else. Check: is form data sent somewhere?
4. **Unhandled fetch responses**: fetch or axios calls where the response is not processed. Check: is the response parsed and used?
5. **Console.log-only functions**: Functions whose only real logic is console.log. Check: does the function have side effects beyond logging?
6. **Placeholder text**: UI elements showing "Lorem ipsum", "TODO", "Coming soon", or "Not implemented". Check: is real content rendered?
7. **Commented-out implementations**: Functions with the real logic commented out and a simplified stub active. Check: is the uncommented code the real implementation?
8. **Pass-through functions**: Functions that receive arguments and return them unchanged, or return a hardcoded value regardless of input. Check: does the function transform its inputs?
9. **Empty event handlers**: onClick, onChange, onSubmit handlers that are empty or only log. Check: does the handler cause a state change or side effect?
10. **Mock data in production code**: Test fixtures or seed data used as the actual data source. Check: is data fetched from a real source?

For each stub found: report the file:line, the pattern matched, and the severity (HIGH if it's a must-have acceptance criterion, MEDIUM otherwise).

### Section 6: Verification Result Format
Write heading `## Verification Result Format`. Define the standard output format:
- Summary section: deterministic tool count and results, AI criteria count and concerns, stub count by severity, overall PASS or FAIL
- Deterministic results section: per-tool details
- AI verification section: per-criterion assessment
- Stub findings section: per-stub report with file:line
- Overall determination: PASS requires all deterministic tools pass AND no high-severity stubs AND AI verification finds no critical gaps

Target ~250 lines.

## ESTIMATED_DIFF
~250 lines

## RISKS
- Stub detection patterns may have false positives for legitimately minimal code
- Tooling detection relies on config.json being accurate (set at init time)
- AI verification quality varies with model capability

## ROLLBACK
- git revert <task commit>
